{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "import requests\n",
    "import io\n",
    "\n",
    "import pandas\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve from Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "    \n",
    "This requires the user to set up a .yaml authentication file, with a *kaggle* field that contains within it *UserName* and *Password* (at least, that is what I've named my credential variables; I am not sure that they have to be named this way)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_kaggle_csv(url, creds_path='auth.yaml'):\n",
    "    # get creds\n",
    "    with open(creds_path) as f:\n",
    "        creds = yaml.load(f)['kaggle']\n",
    "        \n",
    "    # initiate request to get file\n",
    "    r = requests.get(url)\n",
    "    \n",
    "    # pass creds and complete retrieval\n",
    "    data = requests.post(r.url,\n",
    "                         data=creds).text\n",
    "    \n",
    "    # return DataFrame of csv\n",
    "    return pandas.read_csv(io.StringIO(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'https://www.kaggle.com/uciml/mushroom-classification/downloads/mushrooms.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = get_kaggle_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>cap-shape</th>\n",
       "      <th>cap-surface</th>\n",
       "      <th>cap-color</th>\n",
       "      <th>bruises</th>\n",
       "      <th>odor</th>\n",
       "      <th>gill-attachment</th>\n",
       "      <th>gill-spacing</th>\n",
       "      <th>gill-size</th>\n",
       "      <th>gill-color</th>\n",
       "      <th>...</th>\n",
       "      <th>stalk-surface-below-ring</th>\n",
       "      <th>stalk-color-above-ring</th>\n",
       "      <th>stalk-color-below-ring</th>\n",
       "      <th>veil-type</th>\n",
       "      <th>veil-color</th>\n",
       "      <th>ring-number</th>\n",
       "      <th>ring-type</th>\n",
       "      <th>spore-print-color</th>\n",
       "      <th>population</th>\n",
       "      <th>habitat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p</td>\n",
       "      <td>x</td>\n",
       "      <td>s</td>\n",
       "      <td>n</td>\n",
       "      <td>t</td>\n",
       "      <td>p</td>\n",
       "      <td>f</td>\n",
       "      <td>c</td>\n",
       "      <td>n</td>\n",
       "      <td>k</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>w</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>k</td>\n",
       "      <td>s</td>\n",
       "      <td>u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e</td>\n",
       "      <td>x</td>\n",
       "      <td>s</td>\n",
       "      <td>y</td>\n",
       "      <td>t</td>\n",
       "      <td>a</td>\n",
       "      <td>f</td>\n",
       "      <td>c</td>\n",
       "      <td>b</td>\n",
       "      <td>k</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>w</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e</td>\n",
       "      <td>b</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>t</td>\n",
       "      <td>l</td>\n",
       "      <td>f</td>\n",
       "      <td>c</td>\n",
       "      <td>b</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>w</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p</td>\n",
       "      <td>x</td>\n",
       "      <td>y</td>\n",
       "      <td>w</td>\n",
       "      <td>t</td>\n",
       "      <td>p</td>\n",
       "      <td>f</td>\n",
       "      <td>c</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>w</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>k</td>\n",
       "      <td>s</td>\n",
       "      <td>u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e</td>\n",
       "      <td>x</td>\n",
       "      <td>s</td>\n",
       "      <td>g</td>\n",
       "      <td>f</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>w</td>\n",
       "      <td>b</td>\n",
       "      <td>k</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>w</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>o</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>a</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  class cap-shape cap-surface cap-color bruises odor gill-attachment  \\\n",
       "0     p         x           s         n       t    p               f   \n",
       "1     e         x           s         y       t    a               f   \n",
       "2     e         b           s         w       t    l               f   \n",
       "3     p         x           y         w       t    p               f   \n",
       "4     e         x           s         g       f    n               f   \n",
       "\n",
       "  gill-spacing gill-size gill-color   ...   stalk-surface-below-ring  \\\n",
       "0            c         n          k   ...                          s   \n",
       "1            c         b          k   ...                          s   \n",
       "2            c         b          n   ...                          s   \n",
       "3            c         n          n   ...                          s   \n",
       "4            w         b          k   ...                          s   \n",
       "\n",
       "  stalk-color-above-ring stalk-color-below-ring veil-type veil-color  \\\n",
       "0                      w                      w         p          w   \n",
       "1                      w                      w         p          w   \n",
       "2                      w                      w         p          w   \n",
       "3                      w                      w         p          w   \n",
       "4                      w                      w         p          w   \n",
       "\n",
       "  ring-number ring-type spore-print-color population habitat  \n",
       "0           o         p                 k          s       u  \n",
       "1           o         p                 n          n       g  \n",
       "2           o         p                 n          n       m  \n",
       "3           o         p                 k          s       u  \n",
       "4           o         e                 n          a       g  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check length; check for missing or duplicated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8124"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class                       0\n",
       "cap-shape                   0\n",
       "cap-surface                 0\n",
       "cap-color                   0\n",
       "bruises                     0\n",
       "odor                        0\n",
       "gill-attachment             0\n",
       "gill-spacing                0\n",
       "gill-size                   0\n",
       "gill-color                  0\n",
       "stalk-shape                 0\n",
       "stalk-root                  0\n",
       "stalk-surface-above-ring    0\n",
       "stalk-surface-below-ring    0\n",
       "stalk-color-above-ring      0\n",
       "stalk-color-below-ring      0\n",
       "veil-type                   0\n",
       "veil-color                  0\n",
       "ring-number                 0\n",
       "ring-type                   0\n",
       "spore-print-color           0\n",
       "population                  0\n",
       "habitat                     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[data.duplicated()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confirm all data is categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p' 'e']\n",
      "['x' 'b' 's' 'f' 'k' 'c']\n",
      "['s' 'y' 'f' 'g']\n",
      "['n' 'y' 'w' 'g' 'e' 'p' 'b' 'u' 'c' 'r']\n",
      "['t' 'f']\n",
      "['p' 'a' 'l' 'n' 'f' 'c' 'y' 's' 'm']\n",
      "['f' 'a']\n",
      "['c' 'w']\n",
      "['n' 'b']\n",
      "['k' 'n' 'g' 'p' 'w' 'h' 'u' 'e' 'b' 'r' 'y' 'o']\n",
      "['e' 't']\n",
      "['e' 'c' 'b' 'r' '?']\n",
      "['s' 'f' 'k' 'y']\n",
      "['s' 'f' 'y' 'k']\n",
      "['w' 'g' 'p' 'n' 'b' 'e' 'o' 'c' 'y']\n",
      "['w' 'p' 'g' 'b' 'n' 'e' 'y' 'o' 'c']\n",
      "['p']\n",
      "['w' 'n' 'o' 'y']\n",
      "['o' 't' 'n']\n",
      "['p' 'e' 'l' 'f' 'n']\n",
      "['k' 'n' 'u' 'h' 'w' 'r' 'o' 'y' 'b']\n",
      "['s' 'n' 'a' 'v' 'y' 'c']\n",
      "['u' 'g' 'm' 'd' 'p' 'w' 'l']\n"
     ]
    }
   ],
   "source": [
    "for col in data.columns:\n",
    "    print(data[col].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at counts of *edible* vs. *poisonous*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "e    4208\n",
       "p    3916\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Exploratory Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.A. Quick check for which features might provide the most insight "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def diff_between_groups(group_1, group_2):\n",
    "    # outer join the two groups into one DataFrame\n",
    "    # fill missing values with 0\n",
    "    grouped = group_1.to_frame().join(group_2, \n",
    "                                      rsuffix='_', \n",
    "                                      how='outer').fillna(0)\n",
    "    \n",
    "    # scale counts to be proportionte to the total in each group\n",
    "    grouped = grouped / grouped.sum()\n",
    "    \n",
    "    # get difference between scaled values in each group\n",
    "    diff = grouped[grouped.columns[0]] - \\\n",
    "                grouped[grouped.columns[1]]\n",
    "    \n",
    "    # return average squared differences\n",
    "    return (diff ** 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_groups(df, col):\n",
    "    # return dataframe[col] filtered by each class value\n",
    "    return df[df['class']=='e'][col].value_counts(), \\\n",
    "                df[df['class']=='p'][col].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get differences, sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "diff = []\n",
    "for col in data.columns[1:]:\n",
    "    diff.append((col, \n",
    "                 diff_between_groups(*get_groups(data, \n",
    "                                               col))))\n",
    "\n",
    "sorted_diff = \\\n",
    "        sorted(diff, \n",
    "               key = lambda tup: tup[1],\n",
    "               reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at top 5 most different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gill-size', 0.24948565575061857),\n",
       " ('bruises', 0.24467478772253914),\n",
       " ('stalk-surface-above-ring', 0.128270415429778),\n",
       " ('stalk-surface-below-ring', 0.11163432018615214),\n",
       " ('odor', 0.10884075521038676)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_diff[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look further into top 2 most different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "group_1, group_2 = get_groups(data, 'gill-size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "e  b    3920\n",
       "   n     288\n",
       "p  n    2224\n",
       "   b    1692\n",
       "Name: gill-size, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas.concat([pandas.concat({'e': group_1}),\n",
    "               pandas.concat({'p': group_2})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "group_1, group_2 = get_groups(data, 'bruises')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "e  t    2752\n",
       "   f    1456\n",
       "p  f    3292\n",
       "   t     624\n",
       "Name: bruises, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas.concat([pandas.concat({'e': group_1}),\n",
    "               pandas.concat({'p': group_2})])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.B. Visualize the relationsips between each of *gill-sizes* and *bruises*, and *class*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize relationship between *gill-size* and *class*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "group_1, group_2 = get_groups(data, 'gill-size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x110868d30>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA5cAAAKpCAYAAAAoktiMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAIBi4hIAAIBi4hIAAIBi4hIAAIBi/w/9EgVTqWNYogAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110760898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = group_1.plot(kind='bar',\n",
    "                  figsize=(11, 8),\n",
    "                  color='g',\n",
    "                  alpha=.65,\n",
    "                  position=0,\n",
    "                  width=.25,\n",
    "                  label='edible')\n",
    "\n",
    "group_2.plot(kind='bar',\n",
    "              figsize=(11, 8),\n",
    "              color='purple',\n",
    "              alpha=.65,\n",
    "              position=1,\n",
    "              width=.25,\n",
    "              label='poisonous',\n",
    "              ax=ax)\n",
    "\n",
    "ax.set_xlim(right=1.5)\n",
    "plt.legend()\n",
    "plt.title('Gill-size value counts, by class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize relationship between *bruises* and *class*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "group_1, group_2 = get_groups(data, 'bruises')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x11323ff60>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA5cAAAKmCAYAAADZxGpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKWkdvOJR7AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10504c748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = group_1.plot(kind='bar',\n",
    "                  figsize=(11, 8),\n",
    "                  color='g',\n",
    "                  alpha=.65,\n",
    "                  position=0,\n",
    "                  width=.25,\n",
    "                  label='edible')\n",
    "\n",
    "group_2.plot(kind='bar',\n",
    "              figsize=(11, 8),\n",
    "              color='purple',\n",
    "              alpha=.65,\n",
    "              position=1,\n",
    "              width=.25,\n",
    "              label='poisonous',\n",
    "              ax=ax)\n",
    "\n",
    "ax.set_xlim(right=1.5)\n",
    "plt.legend()\n",
    "plt.title('Bruises value counts, by class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "    \n",
    "Since these two features seem to be fairly indicative of whether or not a mushroom is poisonous, let's try to cluster our data set into two groups based on these features. We will then see whether our cluster labels prove informative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.C. Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### one-hot encode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "onehot = pandas.get_dummies(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['class_e', 'class_p', 'cap-shape_b', 'cap-shape_c', 'cap-shape_f',\n",
       "       'cap-shape_k', 'cap-shape_s', 'cap-shape_x', 'cap-surface_f',\n",
       "       'cap-surface_g',\n",
       "       ...\n",
       "       'population_s', 'population_v', 'population_y', 'habitat_d',\n",
       "       'habitat_g', 'habitat_l', 'habitat_m', 'habitat_p', 'habitat_u',\n",
       "       'habitat_w'],\n",
       "      dtype='object', length=119)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_two = [col for col in onehot.columns if 'gill-sizes' in col or 'bruises' in col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-means: *gill-size* and *bruises*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=2, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.fit(onehot[top_two])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['cluster_kmeans_1'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <th>cluster_kmeans_1</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">e</th>\n",
       "      <th>0</th>\n",
       "      <td>0.338749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.179222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">p</th>\n",
       "      <th>0</th>\n",
       "      <td>0.076809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.405219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          counts\n",
       "class cluster_kmeans_1          \n",
       "e     0                 0.338749\n",
       "      1                 0.179222\n",
       "p     0                 0.076809\n",
       "      1                 0.405219"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped = \\\n",
    "    data.groupby(['class', \n",
    "                  'cluster_kmeans_1']).size().to_frame().rename(columns={0: 'counts'})\n",
    "grouped / grouped.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "    \n",
    "It seems that the clusters assigned to our data points come pretty close (no pun intented) to separating the data points based on their class. Let's feed in more features and see if we can get an even better representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-means: all columns except for class columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = [col for col in onehot.columns if 'class' not in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=2, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.fit(onehot[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['cluster_kmeans_2'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <th>cluster_kmeans_2</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">e</th>\n",
       "      <th>0</th>\n",
       "      <td>0.510586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.007386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">p</th>\n",
       "      <th>0</th>\n",
       "      <td>0.100443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.381585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          counts\n",
       "class cluster_kmeans_2          \n",
       "e     0                 0.510586\n",
       "      1                 0.007386\n",
       "p     0                 0.100443\n",
       "      1                 0.381585"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped = \\\n",
    "    data.groupby(['class', \n",
    "                  'cluster_kmeans_2']).size().to_frame().rename(columns={0: 'counts'})\n",
    "grouped / grouped.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "    \n",
    "These results are even better than before. It seems like Euclidean distance here is serving us well. Let's go ahead and try to predict *class* using k-Nearest Neighbor classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Predict *class*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.A. k-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>test_f1score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trial</th>\n",
       "      <th>params</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>n_neighbors: 1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_neighbors: 2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_neighbors: 3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_neighbors: 4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_neighbors: 5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2</th>\n",
       "      <th>n_neighbors: 1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_neighbors: 2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_neighbors: 3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_neighbors: 4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_neighbors: 5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">3</th>\n",
       "      <th>n_neighbors: 1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_neighbors: 2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_neighbors: 3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_neighbors: 4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_neighbors: 5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">4</th>\n",
       "      <th>n_neighbors: 1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_neighbors: 2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_neighbors: 3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_neighbors: 4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_neighbors: 5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">5</th>\n",
       "      <th>n_neighbors: 1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_neighbors: 2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_neighbors: 3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_neighbors: 4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_neighbors: 5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      train_accuracy  test_accuracy  test_precision  \\\n",
       "trial params                                                          \n",
       "1     n_neighbors: 1             1.0            1.0             1.0   \n",
       "      n_neighbors: 2             1.0            1.0             1.0   \n",
       "      n_neighbors: 3             1.0            1.0             1.0   \n",
       "      n_neighbors: 4             1.0            1.0             1.0   \n",
       "      n_neighbors: 5             1.0            1.0             1.0   \n",
       "2     n_neighbors: 1             1.0            1.0             1.0   \n",
       "      n_neighbors: 2             1.0            1.0             1.0   \n",
       "      n_neighbors: 3             1.0            1.0             1.0   \n",
       "      n_neighbors: 4             1.0            1.0             1.0   \n",
       "      n_neighbors: 5             1.0            1.0             1.0   \n",
       "3     n_neighbors: 1             1.0            1.0             1.0   \n",
       "      n_neighbors: 2             1.0            1.0             1.0   \n",
       "      n_neighbors: 3             1.0            1.0             1.0   \n",
       "      n_neighbors: 4             1.0            1.0             1.0   \n",
       "      n_neighbors: 5             1.0            1.0             1.0   \n",
       "4     n_neighbors: 1             1.0            1.0             1.0   \n",
       "      n_neighbors: 2             1.0            1.0             1.0   \n",
       "      n_neighbors: 3             1.0            1.0             1.0   \n",
       "      n_neighbors: 4             1.0            1.0             1.0   \n",
       "      n_neighbors: 5             1.0            1.0             1.0   \n",
       "5     n_neighbors: 1             1.0            1.0             1.0   \n",
       "      n_neighbors: 2             1.0            1.0             1.0   \n",
       "      n_neighbors: 3             1.0            1.0             1.0   \n",
       "      n_neighbors: 4             1.0            1.0             1.0   \n",
       "      n_neighbors: 5             1.0            1.0             1.0   \n",
       "\n",
       "                      test_recall  test_f1score  \n",
       "trial params                                     \n",
       "1     n_neighbors: 1          1.0           1.0  \n",
       "      n_neighbors: 2          1.0           1.0  \n",
       "      n_neighbors: 3          1.0           1.0  \n",
       "      n_neighbors: 4          1.0           1.0  \n",
       "      n_neighbors: 5          1.0           1.0  \n",
       "2     n_neighbors: 1          1.0           1.0  \n",
       "      n_neighbors: 2          1.0           1.0  \n",
       "      n_neighbors: 3          1.0           1.0  \n",
       "      n_neighbors: 4          1.0           1.0  \n",
       "      n_neighbors: 5          1.0           1.0  \n",
       "3     n_neighbors: 1          1.0           1.0  \n",
       "      n_neighbors: 2          1.0           1.0  \n",
       "      n_neighbors: 3          1.0           1.0  \n",
       "      n_neighbors: 4          1.0           1.0  \n",
       "      n_neighbors: 5          1.0           1.0  \n",
       "4     n_neighbors: 1          1.0           1.0  \n",
       "      n_neighbors: 2          1.0           1.0  \n",
       "      n_neighbors: 3          1.0           1.0  \n",
       "      n_neighbors: 4          1.0           1.0  \n",
       "      n_neighbors: 5          1.0           1.0  \n",
       "5     n_neighbors: 1          1.0           1.0  \n",
       "      n_neighbors: 2          1.0           1.0  \n",
       "      n_neighbors: 3          1.0           1.0  \n",
       "      n_neighbors: 4          1.0           1.0  \n",
       "      n_neighbors: 5          1.0           1.0  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation.five_fold_validation(KNeighborsClassifier, \n",
    "                                onehot[cols],\n",
    "                                onehot['class_p'],\n",
    "                                'clf',\n",
    "                                [{'n_neighbors': x} for x in range(1, 6)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "    \n",
    "These results are ridiculously good. The fact that test accuracy was just as high as training accuracy would usually be a good indicator of a model's non-overfitedness (sorry, that was not very eloquent), but the fact that this model is literally perfect is a bit worrying. \n",
    "\n",
    "It could be that this data set is just very clean and robust, and that *class* is well-predicted via k-Nearest Neighbor. Unfortunately I know almost nothing about mushrooms, so any insight that might typically be gleaned through an understanding of the problem space is at this point not within reach. \n",
    "\n",
    "It would be good to do some further investigations. We could reduce complexity by increasing the number of neighbors considered, and we could also reduce the number of folds tested across to allow for more test data. But, this model is already taking some time to test as is, so let's move on to some other model families."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.B. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>test_f1score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trial</th>\n",
       "      <th>params</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>Default</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>Default</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <th>Default</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th>Default</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <th>Default</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.998153</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.996169</td>\n",
       "      <td>0.998081</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               train_accuracy  test_accuracy  test_precision  test_recall  \\\n",
       "trial params                                                                \n",
       "1     Default             1.0       1.000000             1.0     1.000000   \n",
       "2     Default             1.0       1.000000             1.0     1.000000   \n",
       "3     Default             1.0       1.000000             1.0     1.000000   \n",
       "4     Default             1.0       1.000000             1.0     1.000000   \n",
       "5     Default             1.0       0.998153             1.0     0.996169   \n",
       "\n",
       "               test_f1score  \n",
       "trial params                 \n",
       "1     Default      1.000000  \n",
       "2     Default      1.000000  \n",
       "3     Default      1.000000  \n",
       "4     Default      1.000000  \n",
       "5     Default      0.998081  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation.five_fold_validation(LogisticRegression, \n",
    "                                onehot[cols],\n",
    "                                onehot['class_p'],\n",
    "                                'clf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "    \n",
    "Using a linear model has also given perfect or near-perfect results. Let's try to reduce the number of features used and see if we can still make good predicitions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_cols = [col for col in onehot.columns if 'gill-size' in col or 'bruises' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>test_f1score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trial</th>\n",
       "      <th>params</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>Default</th>\n",
       "      <td>0.761619</td>\n",
       "      <td>0.776753</td>\n",
       "      <td>0.707799</td>\n",
       "      <td>0.914541</td>\n",
       "      <td>0.797997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>Default</th>\n",
       "      <td>0.766118</td>\n",
       "      <td>0.758769</td>\n",
       "      <td>0.688162</td>\n",
       "      <td>0.913155</td>\n",
       "      <td>0.784852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <th>Default</th>\n",
       "      <td>0.767195</td>\n",
       "      <td>0.754462</td>\n",
       "      <td>0.688235</td>\n",
       "      <td>0.896552</td>\n",
       "      <td>0.778702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th>Default</th>\n",
       "      <td>0.756462</td>\n",
       "      <td>0.755542</td>\n",
       "      <td>0.889113</td>\n",
       "      <td>0.563218</td>\n",
       "      <td>0.689601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <th>Default</th>\n",
       "      <td>0.760154</td>\n",
       "      <td>0.740764</td>\n",
       "      <td>0.863454</td>\n",
       "      <td>0.549170</td>\n",
       "      <td>0.671351</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               train_accuracy  test_accuracy  test_precision  test_recall  \\\n",
       "trial params                                                                \n",
       "1     Default        0.761619       0.776753        0.707799     0.914541   \n",
       "2     Default        0.766118       0.758769        0.688162     0.913155   \n",
       "3     Default        0.767195       0.754462        0.688235     0.896552   \n",
       "4     Default        0.756462       0.755542        0.889113     0.563218   \n",
       "5     Default        0.760154       0.740764        0.863454     0.549170   \n",
       "\n",
       "               test_f1score  \n",
       "trial params                 \n",
       "1     Default      0.797997  \n",
       "2     Default      0.784852  \n",
       "3     Default      0.778702  \n",
       "4     Default      0.689601  \n",
       "5     Default      0.671351  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation.five_fold_validation(LogisticRegression, \n",
    "                                onehot[new_cols],\n",
    "                                onehot['class_p'],\n",
    "                                'clf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "    \n",
    "Using only *gill-size* and *bruises* to predict class has proven somewhat effective, but we have seen a severe reduction in performance. Interestingly, we can see, given the differences between precision and recall and their disparity across these trials, that there appears to be a distinct portion of the data for which these features are not helpful for classification (which you can see if you look back at our grouping check above).\n",
    "\n",
    "Let's try also including our previously-calculated clusters as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "onehot['cluster_kmeans_2'] = data['cluster_kmeans_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_cols.append('cluster_kmeans_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>test_f1score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trial</th>\n",
       "      <th>params</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>Default</th>\n",
       "      <td>0.912435</td>\n",
       "      <td>0.914514</td>\n",
       "      <td>0.964029</td>\n",
       "      <td>0.854592</td>\n",
       "      <td>0.906018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>Default</th>\n",
       "      <td>0.910294</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.966006</td>\n",
       "      <td>0.871009</td>\n",
       "      <td>0.916051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <th>Default</th>\n",
       "      <td>0.913987</td>\n",
       "      <td>0.908308</td>\n",
       "      <td>0.952857</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.899528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th>Default</th>\n",
       "      <td>0.913692</td>\n",
       "      <td>0.909483</td>\n",
       "      <td>0.945378</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>0.901804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <th>Default</th>\n",
       "      <td>0.913846</td>\n",
       "      <td>0.908867</td>\n",
       "      <td>0.950355</td>\n",
       "      <td>0.855683</td>\n",
       "      <td>0.900538</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               train_accuracy  test_accuracy  test_precision  test_recall  \\\n",
       "trial params                                                                \n",
       "1     Default        0.912435       0.914514        0.964029     0.854592   \n",
       "2     Default        0.910294       0.923077        0.966006     0.871009   \n",
       "3     Default        0.913987       0.908308        0.952857     0.851852   \n",
       "4     Default        0.913692       0.909483        0.945378     0.862069   \n",
       "5     Default        0.913846       0.908867        0.950355     0.855683   \n",
       "\n",
       "               test_f1score  \n",
       "trial params                 \n",
       "1     Default      0.906018  \n",
       "2     Default      0.916051  \n",
       "3     Default      0.899528  \n",
       "4     Default      0.901804  \n",
       "5     Default      0.900538  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation.five_fold_validation(LogisticRegression, \n",
    "                                onehot[new_cols],\n",
    "                                onehot['class_p'],\n",
    "                                'clf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "    \n",
    "Using only these three features (one of which is nevertheless quite derived) has allowed us to consistently classify these mushrooms as poisonous or not with an accuracy of greater than 90%.\n",
    "\n",
    "Interestingly, there is a tangible difference here between the model's precision and recall scores. We can see here that the model is careful when it comes to making falsely positive classifications. But that comes at the expense of its carefulness with respect to making falsely negative classifications. Seeing as how, based on how I've set this up, *poisonous* is our positive class, we would want to be careful using this model for the purpose of deciding whether or not to eat a mushroom. \n",
    "\n",
    "Just for fun, let's try to classify based on only a datapoint's cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>test_f1score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trial</th>\n",
       "      <th>params</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>Default</th>\n",
       "      <td>0.892890</td>\n",
       "      <td>0.889299</td>\n",
       "      <td>0.985531</td>\n",
       "      <td>0.781888</td>\n",
       "      <td>0.871977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>Default</th>\n",
       "      <td>0.890445</td>\n",
       "      <td>0.899077</td>\n",
       "      <td>0.984351</td>\n",
       "      <td>0.803321</td>\n",
       "      <td>0.884669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <th>Default</th>\n",
       "      <td>0.893060</td>\n",
       "      <td>0.888615</td>\n",
       "      <td>0.983923</td>\n",
       "      <td>0.781609</td>\n",
       "      <td>0.871174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th>Default</th>\n",
       "      <td>0.893231</td>\n",
       "      <td>0.887931</td>\n",
       "      <td>0.967341</td>\n",
       "      <td>0.794381</td>\n",
       "      <td>0.872370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <th>Default</th>\n",
       "      <td>0.891231</td>\n",
       "      <td>0.895936</td>\n",
       "      <td>0.984227</td>\n",
       "      <td>0.796935</td>\n",
       "      <td>0.880734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               train_accuracy  test_accuracy  test_precision  test_recall  \\\n",
       "trial params                                                                \n",
       "1     Default        0.892890       0.889299        0.985531     0.781888   \n",
       "2     Default        0.890445       0.899077        0.984351     0.803321   \n",
       "3     Default        0.893060       0.888615        0.983923     0.781609   \n",
       "4     Default        0.893231       0.887931        0.967341     0.794381   \n",
       "5     Default        0.891231       0.895936        0.984227     0.796935   \n",
       "\n",
       "               test_f1score  \n",
       "trial params                 \n",
       "1     Default      0.871977  \n",
       "2     Default      0.884669  \n",
       "3     Default      0.871174  \n",
       "4     Default      0.872370  \n",
       "5     Default      0.880734  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation.five_fold_validation(LogisticRegression, \n",
    "                                onehot['cluster_kmeans_2'].to_frame(),\n",
    "                                onehot['class_p'],\n",
    "                                'clf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "    \n",
    "Our accuracy has dipped slightly, though it is still close to 90%. But we can see that the disparity between precision and recall has become much more pronouced.\n",
    "\n",
    "Let's go ahead and implement some regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>test_f1score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trial</th>\n",
       "      <th>params</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">1</th>\n",
       "      <th>C: 0.01</th>\n",
       "      <td>0.985072</td>\n",
       "      <td>0.980320</td>\n",
       "      <td>0.989583</td>\n",
       "      <td>0.969388</td>\n",
       "      <td>0.979381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C: 0.1</th>\n",
       "      <td>0.998307</td>\n",
       "      <td>0.999385</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998724</td>\n",
       "      <td>0.999362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C: 1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">2</th>\n",
       "      <th>C: 0.01</th>\n",
       "      <td>0.983690</td>\n",
       "      <td>0.987077</td>\n",
       "      <td>0.990979</td>\n",
       "      <td>0.982120</td>\n",
       "      <td>0.986530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C: 0.1</th>\n",
       "      <td>0.998923</td>\n",
       "      <td>0.999385</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998723</td>\n",
       "      <td>0.999361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C: 1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">3</th>\n",
       "      <th>C: 0.01</th>\n",
       "      <td>0.984459</td>\n",
       "      <td>0.983385</td>\n",
       "      <td>0.989637</td>\n",
       "      <td>0.975734</td>\n",
       "      <td>0.982637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C: 0.1</th>\n",
       "      <td>0.999077</td>\n",
       "      <td>0.998154</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.996169</td>\n",
       "      <td>0.998081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C: 1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">4</th>\n",
       "      <th>C: 0.01</th>\n",
       "      <td>0.983846</td>\n",
       "      <td>0.985222</td>\n",
       "      <td>0.988417</td>\n",
       "      <td>0.980843</td>\n",
       "      <td>0.984615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C: 0.1</th>\n",
       "      <td>0.998923</td>\n",
       "      <td>0.999384</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998723</td>\n",
       "      <td>0.999361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C: 1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">5</th>\n",
       "      <th>C: 0.01</th>\n",
       "      <td>0.984769</td>\n",
       "      <td>0.983990</td>\n",
       "      <td>0.994771</td>\n",
       "      <td>0.971903</td>\n",
       "      <td>0.983204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C: 0.1</th>\n",
       "      <td>0.998923</td>\n",
       "      <td>0.995690</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.991060</td>\n",
       "      <td>0.995510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C: 1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998153</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.996169</td>\n",
       "      <td>0.998081</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               train_accuracy  test_accuracy  test_precision  test_recall  \\\n",
       "trial params                                                                \n",
       "1     C: 0.01        0.985072       0.980320        0.989583     0.969388   \n",
       "      C: 0.1         0.998307       0.999385        1.000000     0.998724   \n",
       "      C: 1           1.000000       1.000000        1.000000     1.000000   \n",
       "2     C: 0.01        0.983690       0.987077        0.990979     0.982120   \n",
       "      C: 0.1         0.998923       0.999385        1.000000     0.998723   \n",
       "      C: 1           1.000000       1.000000        1.000000     1.000000   \n",
       "3     C: 0.01        0.984459       0.983385        0.989637     0.975734   \n",
       "      C: 0.1         0.999077       0.998154        1.000000     0.996169   \n",
       "      C: 1           1.000000       1.000000        1.000000     1.000000   \n",
       "4     C: 0.01        0.983846       0.985222        0.988417     0.980843   \n",
       "      C: 0.1         0.998923       0.999384        1.000000     0.998723   \n",
       "      C: 1           1.000000       1.000000        1.000000     1.000000   \n",
       "5     C: 0.01        0.984769       0.983990        0.994771     0.971903   \n",
       "      C: 0.1         0.998923       0.995690        1.000000     0.991060   \n",
       "      C: 1           1.000000       0.998153        1.000000     0.996169   \n",
       "\n",
       "               test_f1score  \n",
       "trial params                 \n",
       "1     C: 0.01      0.979381  \n",
       "      C: 0.1       0.999362  \n",
       "      C: 1         1.000000  \n",
       "2     C: 0.01      0.986530  \n",
       "      C: 0.1       0.999361  \n",
       "      C: 1         1.000000  \n",
       "3     C: 0.01      0.982637  \n",
       "      C: 0.1       0.998081  \n",
       "      C: 1         1.000000  \n",
       "4     C: 0.01      0.984615  \n",
       "      C: 0.1       0.999361  \n",
       "      C: 1         1.000000  \n",
       "5     C: 0.01      0.983204  \n",
       "      C: 0.1       0.995510  \n",
       "      C: 1         0.998081  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation.five_fold_validation(LogisticRegression, \n",
    "                                onehot[cols],\n",
    "                                onehot['class_p'],\n",
    "                                'clf',\n",
    "                                [{'C': x} for x in (.01, .1, 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "    \n",
    "Even if we force our coefficients to stay near 0, we still see Logistic Regression performing quite well. \n",
    "\n",
    "An L2 regularization of .01 indicates some generalizability to the model. Additionally, we can see a slight tendancy towards precision, at the expsense of recall, but overall this model seems to be performing in a balanced fashion.\n",
    "\n",
    "What if we allow some coefficients to go all the way to 0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>test_f1score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trial</th>\n",
       "      <th>params</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">1</th>\n",
       "      <th>C: 0.01, penalty: l1</th>\n",
       "      <td>0.977224</td>\n",
       "      <td>0.977860</td>\n",
       "      <td>0.979487</td>\n",
       "      <td>0.974490</td>\n",
       "      <td>0.976982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C: 0.1, penalty: l1</th>\n",
       "      <td>0.997999</td>\n",
       "      <td>0.998155</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.996173</td>\n",
       "      <td>0.998083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C: 1, penalty: l1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">2</th>\n",
       "      <th>C: 0.01, penalty: l1</th>\n",
       "      <td>0.975996</td>\n",
       "      <td>0.982769</td>\n",
       "      <td>0.982120</td>\n",
       "      <td>0.982120</td>\n",
       "      <td>0.982120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C: 0.1, penalty: l1</th>\n",
       "      <td>0.997846</td>\n",
       "      <td>0.998769</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997446</td>\n",
       "      <td>0.998721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C: 1, penalty: l1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">3</th>\n",
       "      <th>C: 0.01, penalty: l1</th>\n",
       "      <td>0.977997</td>\n",
       "      <td>0.974769</td>\n",
       "      <td>0.970812</td>\n",
       "      <td>0.977011</td>\n",
       "      <td>0.973902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C: 0.1, penalty: l1</th>\n",
       "      <td>0.998000</td>\n",
       "      <td>0.998154</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.996169</td>\n",
       "      <td>0.998081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C: 1, penalty: l1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">4</th>\n",
       "      <th>C: 0.01, penalty: l1</th>\n",
       "      <td>0.976769</td>\n",
       "      <td>0.979680</td>\n",
       "      <td>0.977099</td>\n",
       "      <td>0.980843</td>\n",
       "      <td>0.978967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C: 0.1, penalty: l1</th>\n",
       "      <td>0.997692</td>\n",
       "      <td>0.999384</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998723</td>\n",
       "      <td>0.999361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C: 1, penalty: l1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">5</th>\n",
       "      <th>C: 0.01, penalty: l1</th>\n",
       "      <td>0.978769</td>\n",
       "      <td>0.971675</td>\n",
       "      <td>0.968234</td>\n",
       "      <td>0.973180</td>\n",
       "      <td>0.970701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C: 0.1, penalty: l1</th>\n",
       "      <td>0.998615</td>\n",
       "      <td>0.995690</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.991060</td>\n",
       "      <td>0.995510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C: 1, penalty: l1</th>\n",
       "      <td>0.999846</td>\n",
       "      <td>0.998153</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.996169</td>\n",
       "      <td>0.998081</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            train_accuracy  test_accuracy  test_precision  \\\n",
       "trial params                                                                \n",
       "1     C: 0.01, penalty: l1        0.977224       0.977860        0.979487   \n",
       "      C: 0.1, penalty: l1         0.997999       0.998155        1.000000   \n",
       "      C: 1, penalty: l1           1.000000       1.000000        1.000000   \n",
       "2     C: 0.01, penalty: l1        0.975996       0.982769        0.982120   \n",
       "      C: 0.1, penalty: l1         0.997846       0.998769        1.000000   \n",
       "      C: 1, penalty: l1           1.000000       1.000000        1.000000   \n",
       "3     C: 0.01, penalty: l1        0.977997       0.974769        0.970812   \n",
       "      C: 0.1, penalty: l1         0.998000       0.998154        1.000000   \n",
       "      C: 1, penalty: l1           1.000000       1.000000        1.000000   \n",
       "4     C: 0.01, penalty: l1        0.976769       0.979680        0.977099   \n",
       "      C: 0.1, penalty: l1         0.997692       0.999384        1.000000   \n",
       "      C: 1, penalty: l1           1.000000       1.000000        1.000000   \n",
       "5     C: 0.01, penalty: l1        0.978769       0.971675        0.968234   \n",
       "      C: 0.1, penalty: l1         0.998615       0.995690        1.000000   \n",
       "      C: 1, penalty: l1           0.999846       0.998153        1.000000   \n",
       "\n",
       "                            test_recall  test_f1score  \n",
       "trial params                                           \n",
       "1     C: 0.01, penalty: l1     0.974490      0.976982  \n",
       "      C: 0.1, penalty: l1      0.996173      0.998083  \n",
       "      C: 1, penalty: l1        1.000000      1.000000  \n",
       "2     C: 0.01, penalty: l1     0.982120      0.982120  \n",
       "      C: 0.1, penalty: l1      0.997446      0.998721  \n",
       "      C: 1, penalty: l1        1.000000      1.000000  \n",
       "3     C: 0.01, penalty: l1     0.977011      0.973902  \n",
       "      C: 0.1, penalty: l1      0.996169      0.998081  \n",
       "      C: 1, penalty: l1        1.000000      1.000000  \n",
       "4     C: 0.01, penalty: l1     0.980843      0.978967  \n",
       "      C: 0.1, penalty: l1      0.998723      0.999361  \n",
       "      C: 1, penalty: l1        1.000000      1.000000  \n",
       "5     C: 0.01, penalty: l1     0.973180      0.970701  \n",
       "      C: 0.1, penalty: l1      0.991060      0.995510  \n",
       "      C: 1, penalty: l1        0.996169      0.998081  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation.five_fold_validation(LogisticRegression, \n",
    "                                onehot[cols],\n",
    "                                onehot['class_p'],\n",
    "                                'clf',\n",
    "                                [{'C': x, 'penalty': 'l1'} for x in (.01, .1, 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "\n",
    "Applying L1 regularization yields similar results to those produced with L2 regularization, though for the same regularization values L2 seems to yield slightly better results.\n",
    "    \n",
    "The results produced via Logistic Regression look very promising. Passing all of the dataset's original columns (excluding *class* of course) as features has produced a model that is maybe just the smidgeniest of smidgens less accurate than k-Nearest Neighbor, while testing at a higher speed. Applying regularization, which reduces model complexity and increases the model's generalizability, still resulted in a very accurate model. At this point, it might be nice to give this one a 'good job!' and move on to some other experimentations.\n",
    "\n",
    "The binomial nature of our various features means that this data could possibly be well-classified by a Bernoulli Naive Bayes classifier, so let's go ahead and give that a try. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.C. Navie Bayes: Bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>test_f1score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trial</th>\n",
       "      <th>params</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">1</th>\n",
       "      <th>alpha: 0.01</th>\n",
       "      <td>0.976608</td>\n",
       "      <td>0.976015</td>\n",
       "      <td>0.997330</td>\n",
       "      <td>0.952806</td>\n",
       "      <td>0.974560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha: 0.1</th>\n",
       "      <td>0.958295</td>\n",
       "      <td>0.958180</td>\n",
       "      <td>0.995845</td>\n",
       "      <td>0.917092</td>\n",
       "      <td>0.954847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha: 1</th>\n",
       "      <td>0.941982</td>\n",
       "      <td>0.931734</td>\n",
       "      <td>0.989811</td>\n",
       "      <td>0.867347</td>\n",
       "      <td>0.924541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha: 10</th>\n",
       "      <td>0.925362</td>\n",
       "      <td>0.912669</td>\n",
       "      <td>0.983434</td>\n",
       "      <td>0.832908</td>\n",
       "      <td>0.901934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">2</th>\n",
       "      <th>alpha: 0.01</th>\n",
       "      <td>0.977073</td>\n",
       "      <td>0.980923</td>\n",
       "      <td>0.989583</td>\n",
       "      <td>0.970626</td>\n",
       "      <td>0.980013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha: 0.1</th>\n",
       "      <td>0.958455</td>\n",
       "      <td>0.964923</td>\n",
       "      <td>0.989218</td>\n",
       "      <td>0.937420</td>\n",
       "      <td>0.962623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha: 1</th>\n",
       "      <td>0.938760</td>\n",
       "      <td>0.947077</td>\n",
       "      <td>0.986053</td>\n",
       "      <td>0.902937</td>\n",
       "      <td>0.942667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha: 10</th>\n",
       "      <td>0.920449</td>\n",
       "      <td>0.927385</td>\n",
       "      <td>0.975680</td>\n",
       "      <td>0.871009</td>\n",
       "      <td>0.920378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">3</th>\n",
       "      <th>alpha: 0.01</th>\n",
       "      <td>0.975842</td>\n",
       "      <td>0.979692</td>\n",
       "      <td>0.992126</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.978641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha: 0.1</th>\n",
       "      <td>0.959224</td>\n",
       "      <td>0.958154</td>\n",
       "      <td>0.990398</td>\n",
       "      <td>0.922095</td>\n",
       "      <td>0.955026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha: 1</th>\n",
       "      <td>0.940299</td>\n",
       "      <td>0.937231</td>\n",
       "      <td>0.984353</td>\n",
       "      <td>0.883780</td>\n",
       "      <td>0.931359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha: 10</th>\n",
       "      <td>0.920911</td>\n",
       "      <td>0.921231</td>\n",
       "      <td>0.973951</td>\n",
       "      <td>0.859515</td>\n",
       "      <td>0.913161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">4</th>\n",
       "      <th>alpha: 0.01</th>\n",
       "      <td>0.978154</td>\n",
       "      <td>0.971675</td>\n",
       "      <td>0.977951</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.970399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha: 0.1</th>\n",
       "      <td>0.959692</td>\n",
       "      <td>0.953202</td>\n",
       "      <td>0.977058</td>\n",
       "      <td>0.924649</td>\n",
       "      <td>0.950131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha: 1</th>\n",
       "      <td>0.941077</td>\n",
       "      <td>0.935345</td>\n",
       "      <td>0.973464</td>\n",
       "      <td>0.890166</td>\n",
       "      <td>0.929953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha: 10</th>\n",
       "      <td>0.921692</td>\n",
       "      <td>0.917488</td>\n",
       "      <td>0.968254</td>\n",
       "      <td>0.856960</td>\n",
       "      <td>0.909214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">5</th>\n",
       "      <th>alpha: 0.01</th>\n",
       "      <td>0.978308</td>\n",
       "      <td>0.978448</td>\n",
       "      <td>0.989529</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.977376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha: 0.1</th>\n",
       "      <td>0.959692</td>\n",
       "      <td>0.964901</td>\n",
       "      <td>0.989218</td>\n",
       "      <td>0.937420</td>\n",
       "      <td>0.962623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha: 1</th>\n",
       "      <td>0.940462</td>\n",
       "      <td>0.947044</td>\n",
       "      <td>0.987413</td>\n",
       "      <td>0.901660</td>\n",
       "      <td>0.942590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha: 10</th>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.927956</td>\n",
       "      <td>0.977077</td>\n",
       "      <td>0.871009</td>\n",
       "      <td>0.920999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   train_accuracy  test_accuracy  test_precision  test_recall  \\\n",
       "trial params                                                                    \n",
       "1     alpha: 0.01        0.976608       0.976015        0.997330     0.952806   \n",
       "      alpha: 0.1         0.958295       0.958180        0.995845     0.917092   \n",
       "      alpha: 1           0.941982       0.931734        0.989811     0.867347   \n",
       "      alpha: 10          0.925362       0.912669        0.983434     0.832908   \n",
       "2     alpha: 0.01        0.977073       0.980923        0.989583     0.970626   \n",
       "      alpha: 0.1         0.958455       0.964923        0.989218     0.937420   \n",
       "      alpha: 1           0.938760       0.947077        0.986053     0.902937   \n",
       "      alpha: 10          0.920449       0.927385        0.975680     0.871009   \n",
       "3     alpha: 0.01        0.975842       0.979692        0.992126     0.965517   \n",
       "      alpha: 0.1         0.959224       0.958154        0.990398     0.922095   \n",
       "      alpha: 1           0.940299       0.937231        0.984353     0.883780   \n",
       "      alpha: 10          0.920911       0.921231        0.973951     0.859515   \n",
       "4     alpha: 0.01        0.978154       0.971675        0.977951     0.962963   \n",
       "      alpha: 0.1         0.959692       0.953202        0.977058     0.924649   \n",
       "      alpha: 1           0.941077       0.935345        0.973464     0.890166   \n",
       "      alpha: 10          0.921692       0.917488        0.968254     0.856960   \n",
       "5     alpha: 0.01        0.978308       0.978448        0.989529     0.965517   \n",
       "      alpha: 0.1         0.959692       0.964901        0.989218     0.937420   \n",
       "      alpha: 1           0.940462       0.947044        0.987413     0.901660   \n",
       "      alpha: 10          0.920000       0.927956        0.977077     0.871009   \n",
       "\n",
       "                   test_f1score  \n",
       "trial params                     \n",
       "1     alpha: 0.01      0.974560  \n",
       "      alpha: 0.1       0.954847  \n",
       "      alpha: 1         0.924541  \n",
       "      alpha: 10        0.901934  \n",
       "2     alpha: 0.01      0.980013  \n",
       "      alpha: 0.1       0.962623  \n",
       "      alpha: 1         0.942667  \n",
       "      alpha: 10        0.920378  \n",
       "3     alpha: 0.01      0.978641  \n",
       "      alpha: 0.1       0.955026  \n",
       "      alpha: 1         0.931359  \n",
       "      alpha: 10        0.913161  \n",
       "4     alpha: 0.01      0.970399  \n",
       "      alpha: 0.1       0.950131  \n",
       "      alpha: 1         0.929953  \n",
       "      alpha: 10        0.909214  \n",
       "5     alpha: 0.01      0.977376  \n",
       "      alpha: 0.1       0.962623  \n",
       "      alpha: 1         0.942590  \n",
       "      alpha: 10        0.920999  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation.five_fold_validation(BernoulliNB, \n",
    "                                onehot[cols],\n",
    "                                onehot['class_p'],\n",
    "                                'clf',\n",
    "                                [{'alpha': x} for x in (.01, .1, 1, 10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "    \n",
    "This model performs well, but not quite as well as either of the previous models we tested. As expected, performance went down as we decreased model complexity. Let's move on to some tree-based models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.D. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>test_f1score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trial</th>\n",
       "      <th>params</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>Default</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>Default</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <th>Default</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th>Default</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <th>Default</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.998153</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.996169</td>\n",
       "      <td>0.998081</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               train_accuracy  test_accuracy  test_precision  test_recall  \\\n",
       "trial params                                                                \n",
       "1     Default             1.0       1.000000             1.0     1.000000   \n",
       "2     Default             1.0       1.000000             1.0     1.000000   \n",
       "3     Default             1.0       1.000000             1.0     1.000000   \n",
       "4     Default             1.0       1.000000             1.0     1.000000   \n",
       "5     Default             1.0       0.998153             1.0     0.996169   \n",
       "\n",
       "               test_f1score  \n",
       "trial params                 \n",
       "1     Default      1.000000  \n",
       "2     Default      1.000000  \n",
       "3     Default      1.000000  \n",
       "4     Default      1.000000  \n",
       "5     Default      0.998081  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation.five_fold_validation(DecisionTreeClassifier, \n",
    "                                onehot[cols],\n",
    "                                onehot['class_p'],\n",
    "                                'clf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "    \n",
    "These results are phenomenal, maybe too phenomenal. Let's limit tree depth and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>test_f1score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trial</th>\n",
       "      <th>params</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>max_depth: 4</th>\n",
       "      <td>0.987073</td>\n",
       "      <td>0.980320</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.959184</td>\n",
       "      <td>0.979167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>max_depth: 4</th>\n",
       "      <td>0.993076</td>\n",
       "      <td>0.995692</td>\n",
       "      <td>0.991139</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <th>max_depth: 4</th>\n",
       "      <td>0.994615</td>\n",
       "      <td>0.989538</td>\n",
       "      <td>0.979950</td>\n",
       "      <td>0.998723</td>\n",
       "      <td>0.989247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th>max_depth: 4</th>\n",
       "      <td>0.993692</td>\n",
       "      <td>0.993227</td>\n",
       "      <td>0.986146</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <th>max_depth: 4</th>\n",
       "      <td>0.994923</td>\n",
       "      <td>0.988300</td>\n",
       "      <td>0.979899</td>\n",
       "      <td>0.996169</td>\n",
       "      <td>0.987967</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    train_accuracy  test_accuracy  test_precision  \\\n",
       "trial params                                                        \n",
       "1     max_depth: 4        0.987073       0.980320        1.000000   \n",
       "2     max_depth: 4        0.993076       0.995692        0.991139   \n",
       "3     max_depth: 4        0.994615       0.989538        0.979950   \n",
       "4     max_depth: 4        0.993692       0.993227        0.986146   \n",
       "5     max_depth: 4        0.994923       0.988300        0.979899   \n",
       "\n",
       "                    test_recall  test_f1score  \n",
       "trial params                                   \n",
       "1     max_depth: 4     0.959184      0.979167  \n",
       "2     max_depth: 4     1.000000      0.995550  \n",
       "3     max_depth: 4     0.998723      0.989247  \n",
       "4     max_depth: 4     1.000000      0.993025  \n",
       "5     max_depth: 4     0.996169      0.987967  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation.five_fold_validation(DecisionTreeClassifier, \n",
    "                                onehot[cols],\n",
    "                                onehot['class_p'],\n",
    "                                'clf',\n",
    "                                [{'max_depth': 4}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "    \n",
    "Accuracy has decreased slightly but is still very, very good. The intermitent disparities between train and test accuracy could indicate a propensity for variance in the predictions / overfitting. Let's train a tree and look at what features it is using to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(onehot[cols], onehot['class_p'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(max_depth=4).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99261447562776961"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "col_importances = []\n",
    "for i, col in enumerate(cols):\n",
    "    col_importances.append((col, clf.feature_importances_[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('odor_n', 0.63537621229013563),\n",
       " ('stalk-root_c', 0.18088877145300622),\n",
       " ('stalk-surface-below-ring_y', 0.099011067162175156),\n",
       " ('spore-print-color_r', 0.034623923396534681),\n",
       " ('odor_a', 0.024676952349263506)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(col_importances, key=lambda tup: tup[1], reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "    \n",
    "The presence, or lack thereof, of the value *n* for feautre *odor* seemed to be quite useful for the Decision Tree. Let's go ahead and train a Random Forest classifier, which, in reducing overfitting, should provide us with more even feature importances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.E. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=10, max_depth=4, n_jobs=-1).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98424421467257506"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "    \n",
    "Our accuracy here is lower than that of the single Decision Tree we trained above. We may be able to increase model accuracy through increasing the number of trees used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100, max_depth=4, n_jobs=-1).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99064500246184151"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "col_importances = []\n",
    "for i, col in enumerate(cols):\n",
    "    col_importances.append((col, clf.feature_importances_[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('odor_n', 0.13361282813574549),\n",
       " ('odor_f', 0.085905623830595648),\n",
       " ('gill-size_n', 0.061650278355427107),\n",
       " ('stalk-surface-above-ring_k', 0.061191344563853514),\n",
       " ('stalk-surface-below-ring_k', 0.053571920521429804)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(col_importances, key=lambda tup: tup[1], reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "    \n",
    "Increasing the number of trees used increased our accuracy noticeably. *odor-n* is still the most important feature, though much less domineering in its importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### All models tested produced results that in many contexts would be considered quite good.\n",
    "\n",
    "This data responded well to utilizations of Euclidean distance. k-Means provided a succfessful avenue to decomposition.\n",
    "\n",
    "k-Nearest Neighbor produced perfect results across multiple tests, but that may have been due to overfitting. Due to the increasing wait times involved in increasing the number of neighbors considered (which improves generalizability), we moved on to other models.\n",
    "\n",
    "Bernoulli Naive Bayes was less accuracte than the other models.\n",
    "\n",
    "A Decision Tree produced high accuracy but may have been overfitting. A Random Forest was then tested. This proved highly accuracte. But the Random Forest is more of a black-box model, as evidenced by the wait involed in even printing out some feature importances.\n",
    "\n",
    "That leaves Logistic Regression. This model proved highly accurate, very tweakable, and quick to train and test. Precomputing a *cluster* feature using k-Means provided an additional, highly predictive feature. As its ubiquity throughout a variety of different modeling applications might suggest, Logistic Regression is a powerful model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
